{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 100\n",
        "context_size = 2  # Number of context words to use\n",
        "num_negative_samples = 5  # Number of negative samples per positive sample\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\n",
        "    \"we are what we repeatedly do excellence then is not an act but a habit\",\n",
        "    \"the only way to do great work is to love what you do\",\n",
        "    \"if you can dream it you can do it\",\n",
        "    \"do not wait to strike till the iron is hot but make it hot by striking\",\n",
        "    \"whether you think you can or you think you cannot you are right\",\n",
        "]"
      ],
      "metadata": {
        "id": "SKPrvr_151Lp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Function: preprocess_corpus\n",
        "# Purpose:  Prepare the text corpus for Word2Vec training by\n",
        "#           - Tokenizing sentences into words\n",
        "#           - Building a vocabulary\n",
        "#           - Creating index mappings (word <-> integer ID)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def preprocess_corpus(corpus):\n",
        "    \"\"\"\n",
        "    corpus : list of strings\n",
        "        Example: [\"he is a king\", \"she is a queen\"]\n",
        "\n",
        "    Returns:\n",
        "        words       -> list of all words in the corpus (with repetition)\n",
        "        word_to_idx -> dictionary mapping each unique word to an integer index\n",
        "        idx_to_word -> reverse dictionary mapping each index to its word\n",
        "    \"\"\"\n",
        "\n",
        "    # üß© 1. Flatten all sentences into a single list of words\n",
        "    # 'sentence.split()' splits each sentence into words\n",
        "    # The nested loop [word for sentence in corpus for word in sentence.split()]\n",
        "    # means: for each sentence in corpus, for each word in that sentence, collect it.\n",
        "    words = [word for sentence in corpus for word in sentence.split()]\n",
        "\n",
        "    # üß† Example result:\n",
        "    # words = ['he', 'is', 'a', 'king', 'she', 'is', 'a', 'queen', ...]\n",
        "\n",
        "    # üß© 2. Create the vocabulary (unique set of words)\n",
        "    # 'set(words)' removes duplicates ‚Üí gives all distinct words in the corpus\n",
        "    vocab = set(words)\n",
        "\n",
        "    # üß© 3. Map each unique word to a unique integer index\n",
        "    # enumerate(vocab) gives (index, word) pairs like (0, 'he'), (1, 'queen'), ...\n",
        "    # we store them in a dictionary: {word: index}\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    # üß© 4. Create the reverse mapping\n",
        "    # This allows us to convert indices back to words\n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "    # üß© 5. Return all 3 structures\n",
        "    # 'words' ‚Üí full tokenized list (for training)\n",
        "    # 'word_to_idx' ‚Üí used to convert words into numeric form\n",
        "    # 'idx_to_word' ‚Üí used to decode numeric predictions back to words\n",
        "    return words, word_to_idx, idx_to_word\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Example usage:\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Call the preprocessing function\n",
        "words, word_to_idx, idx_to_word = preprocess_corpus(corpus)\n",
        "\n",
        "# Now:\n",
        "# words       -> ['he', 'is', 'a', 'king', 'she', 'is', 'a', 'queen', ...]\n",
        "# word_to_idx -> {'he': 0, 'is': 1, 'a': 2, 'king': 3, ...}\n",
        "# idx_to_word -> {0: 'he', 1: 'is', 2: 'a', 3: 'king', ...}\n"
      ],
      "metadata": {
        "id": "NlWnFCON520q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A2DiIvgS52xo",
        "outputId": "9083ffef-9878-48c0-9ec8-c0b2043df074"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we',\n",
              " 'are',\n",
              " 'what',\n",
              " 'we',\n",
              " 'repeatedly',\n",
              " 'do',\n",
              " 'excellence',\n",
              " 'then',\n",
              " 'is',\n",
              " 'not',\n",
              " 'an',\n",
              " 'act',\n",
              " 'but',\n",
              " 'a',\n",
              " 'habit',\n",
              " 'the',\n",
              " 'only',\n",
              " 'way',\n",
              " 'to',\n",
              " 'do',\n",
              " 'great',\n",
              " 'work',\n",
              " 'is',\n",
              " 'to',\n",
              " 'love',\n",
              " 'what',\n",
              " 'you',\n",
              " 'do',\n",
              " 'if',\n",
              " 'you',\n",
              " 'can',\n",
              " 'dream',\n",
              " 'it',\n",
              " 'you',\n",
              " 'can',\n",
              " 'do',\n",
              " 'it',\n",
              " 'do',\n",
              " 'not',\n",
              " 'wait',\n",
              " 'to',\n",
              " 'strike',\n",
              " 'till',\n",
              " 'the',\n",
              " 'iron',\n",
              " 'is',\n",
              " 'hot',\n",
              " 'but',\n",
              " 'make',\n",
              " 'it',\n",
              " 'hot',\n",
              " 'by',\n",
              " 'striking',\n",
              " 'whether',\n",
              " 'you',\n",
              " 'think',\n",
              " 'you',\n",
              " 'can',\n",
              " 'or',\n",
              " 'you',\n",
              " 'think',\n",
              " 'you',\n",
              " 'cannot',\n",
              " 'you',\n",
              " 'are',\n",
              " 'right']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Qm5EBo6P52ua",
        "outputId": "07d9fec4-76db-4ed0-8d78-e1b379242238"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'wait': 0,\n",
              " 'can': 1,\n",
              " 'till': 2,\n",
              " 'the': 3,\n",
              " 'do': 4,\n",
              " 'love': 5,\n",
              " 'whether': 6,\n",
              " 'great': 7,\n",
              " 'it': 8,\n",
              " 'strike': 9,\n",
              " 'hot': 10,\n",
              " 'then': 11,\n",
              " 'cannot': 12,\n",
              " 'think': 13,\n",
              " 'are': 14,\n",
              " 'by': 15,\n",
              " 'to': 16,\n",
              " 'only': 17,\n",
              " 'not': 18,\n",
              " 'or': 19,\n",
              " 'is': 20,\n",
              " 'we': 21,\n",
              " 'way': 22,\n",
              " 'habit': 23,\n",
              " 'an': 24,\n",
              " 'make': 25,\n",
              " 'dream': 26,\n",
              " 'iron': 27,\n",
              " 'you': 28,\n",
              " 'a': 29,\n",
              " 'what': 30,\n",
              " 'striking': 31,\n",
              " 'but': 32,\n",
              " 'work': 33,\n",
              " 'if': 34,\n",
              " 'excellence': 35,\n",
              " 'act': 36,\n",
              " 'right': 37,\n",
              " 'repeatedly': 38}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TUYw4H3G52rS",
        "outputId": "d98f7f6d-925d-45d2-9750-ebbe234230b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'wait',\n",
              " 1: 'can',\n",
              " 2: 'till',\n",
              " 3: 'the',\n",
              " 4: 'do',\n",
              " 5: 'love',\n",
              " 6: 'whether',\n",
              " 7: 'great',\n",
              " 8: 'it',\n",
              " 9: 'strike',\n",
              " 10: 'hot',\n",
              " 11: 'then',\n",
              " 12: 'cannot',\n",
              " 13: 'think',\n",
              " 14: 'are',\n",
              " 15: 'by',\n",
              " 16: 'to',\n",
              " 17: 'only',\n",
              " 18: 'not',\n",
              " 19: 'or',\n",
              " 20: 'is',\n",
              " 21: 'we',\n",
              " 22: 'way',\n",
              " 23: 'habit',\n",
              " 24: 'an',\n",
              " 25: 'make',\n",
              " 26: 'dream',\n",
              " 27: 'iron',\n",
              " 28: 'you',\n",
              " 29: 'a',\n",
              " 30: 'what',\n",
              " 31: 'striking',\n",
              " 32: 'but',\n",
              " 33: 'work',\n",
              " 34: 'if',\n",
              " 35: 'excellence',\n",
              " 36: 'act',\n",
              " 37: 'right',\n",
              " 38: 'repeatedly'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# FUNCTION: generate_training_data\n",
        "# Purpose:\n",
        "#   Create pairs of (target_word, context_word) from a list\n",
        "#   of words, according to a given context window size.\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def generate_training_data(words, word_to_idx, context_size):\n",
        "    # Initialize an empty list to hold all training pairs\n",
        "    # Each item in 'data' will be a tuple: (target_word_index, context_word_index)\n",
        "    data = []\n",
        "\n",
        "    # Loop through each word in the corpus ‚Äî but skip the first and last\n",
        "    # 'context_size' words to avoid index out-of-range errors\n",
        "    for i in range(context_size, len(words) - context_size):\n",
        "        # ---------------------------------------------------------\n",
        "        # Step 1: Identify the 'target word'\n",
        "        # ---------------------------------------------------------\n",
        "        # The word at position 'i' is our center (target) word.\n",
        "        # We convert it to its integer index using 'word_to_idx'.\n",
        "        target_word = word_to_idx[words[i]]\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # Step 2: Collect 'context words' around the target\n",
        "        # ---------------------------------------------------------\n",
        "        # Left-side context words: words[i-1], words[i-2], ... up to context_size\n",
        "        # We move backwards using 'i - j - 1'\n",
        "        left_context = [word_to_idx[words[i - j - 1]] for j in range(context_size)]\n",
        "\n",
        "        # Right-side context words: words[i+1], words[i+2], ... up to context_size\n",
        "        # We move forwards using 'i + j + 1'\n",
        "        right_context = [word_to_idx[words[i + j + 1]] for j in range(context_size)]\n",
        "\n",
        "        # Combine both sides into a single list of all context word indices\n",
        "        context_words = left_context + right_context\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # Step 3: Create training pairs\n",
        "        # ---------------------------------------------------------\n",
        "        # For each context word, we make a (target, context) pair\n",
        "        for context_word in context_words:\n",
        "            data.append((target_word, context_word))\n",
        "\n",
        "    # Return the full list of training pairs\n",
        "    return data\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Example of using the function:\n",
        "# ---------------------------------------------------------\n",
        "# words          ‚Üí flat list of all words from the corpus\n",
        "# word_to_idx    ‚Üí mapping of each word to a unique index\n",
        "# context_size   ‚Üí number of words to take on each side\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "training_data = generate_training_data(words, word_to_idx, context_size)\n"
      ],
      "metadata": {
        "id": "x0f4yz4R52oP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Da4NAA1_7EQ5",
        "outputId": "43f0a5e3-b310-473b-f632-7394e6a05dbb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(30, 14),\n",
              " (30, 21),\n",
              " (30, 21),\n",
              " (30, 38),\n",
              " (21, 30),\n",
              " (21, 14),\n",
              " (21, 38),\n",
              " (21, 4),\n",
              " (38, 21),\n",
              " (38, 30),\n",
              " (38, 4),\n",
              " (38, 35),\n",
              " (4, 38),\n",
              " (4, 21),\n",
              " (4, 35),\n",
              " (4, 11),\n",
              " (35, 4),\n",
              " (35, 38),\n",
              " (35, 11),\n",
              " (35, 20),\n",
              " (11, 35),\n",
              " (11, 4),\n",
              " (11, 20),\n",
              " (11, 18),\n",
              " (20, 11),\n",
              " (20, 35),\n",
              " (20, 18),\n",
              " (20, 24),\n",
              " (18, 20),\n",
              " (18, 11),\n",
              " (18, 24),\n",
              " (18, 36),\n",
              " (24, 18),\n",
              " (24, 20),\n",
              " (24, 36),\n",
              " (24, 32),\n",
              " (36, 24),\n",
              " (36, 18),\n",
              " (36, 32),\n",
              " (36, 29),\n",
              " (32, 36),\n",
              " (32, 24),\n",
              " (32, 29),\n",
              " (32, 23),\n",
              " (29, 32),\n",
              " (29, 36),\n",
              " (29, 23),\n",
              " (29, 3),\n",
              " (23, 29),\n",
              " (23, 32),\n",
              " (23, 3),\n",
              " (23, 17),\n",
              " (3, 23),\n",
              " (3, 29),\n",
              " (3, 17),\n",
              " (3, 22),\n",
              " (17, 3),\n",
              " (17, 23),\n",
              " (17, 22),\n",
              " (17, 16),\n",
              " (22, 17),\n",
              " (22, 3),\n",
              " (22, 16),\n",
              " (22, 4),\n",
              " (16, 22),\n",
              " (16, 17),\n",
              " (16, 4),\n",
              " (16, 7),\n",
              " (4, 16),\n",
              " (4, 22),\n",
              " (4, 7),\n",
              " (4, 33),\n",
              " (7, 4),\n",
              " (7, 16),\n",
              " (7, 33),\n",
              " (7, 20),\n",
              " (33, 7),\n",
              " (33, 4),\n",
              " (33, 20),\n",
              " (33, 16),\n",
              " (20, 33),\n",
              " (20, 7),\n",
              " (20, 16),\n",
              " (20, 5),\n",
              " (16, 20),\n",
              " (16, 33),\n",
              " (16, 5),\n",
              " (16, 30),\n",
              " (5, 16),\n",
              " (5, 20),\n",
              " (5, 30),\n",
              " (5, 28),\n",
              " (30, 5),\n",
              " (30, 16),\n",
              " (30, 28),\n",
              " (30, 4),\n",
              " (28, 30),\n",
              " (28, 5),\n",
              " (28, 4),\n",
              " (28, 34),\n",
              " (4, 28),\n",
              " (4, 30),\n",
              " (4, 34),\n",
              " (4, 28),\n",
              " (34, 4),\n",
              " (34, 28),\n",
              " (34, 28),\n",
              " (34, 1),\n",
              " (28, 34),\n",
              " (28, 4),\n",
              " (28, 1),\n",
              " (28, 26),\n",
              " (1, 28),\n",
              " (1, 34),\n",
              " (1, 26),\n",
              " (1, 8),\n",
              " (26, 1),\n",
              " (26, 28),\n",
              " (26, 8),\n",
              " (26, 28),\n",
              " (8, 26),\n",
              " (8, 1),\n",
              " (8, 28),\n",
              " (8, 1),\n",
              " (28, 8),\n",
              " (28, 26),\n",
              " (28, 1),\n",
              " (28, 4),\n",
              " (1, 28),\n",
              " (1, 8),\n",
              " (1, 4),\n",
              " (1, 8),\n",
              " (4, 1),\n",
              " (4, 28),\n",
              " (4, 8),\n",
              " (4, 4),\n",
              " (8, 4),\n",
              " (8, 1),\n",
              " (8, 4),\n",
              " (8, 18),\n",
              " (4, 8),\n",
              " (4, 4),\n",
              " (4, 18),\n",
              " (4, 0),\n",
              " (18, 4),\n",
              " (18, 8),\n",
              " (18, 0),\n",
              " (18, 16),\n",
              " (0, 18),\n",
              " (0, 4),\n",
              " (0, 16),\n",
              " (0, 9),\n",
              " (16, 0),\n",
              " (16, 18),\n",
              " (16, 9),\n",
              " (16, 2),\n",
              " (9, 16),\n",
              " (9, 0),\n",
              " (9, 2),\n",
              " (9, 3),\n",
              " (2, 9),\n",
              " (2, 16),\n",
              " (2, 3),\n",
              " (2, 27),\n",
              " (3, 2),\n",
              " (3, 9),\n",
              " (3, 27),\n",
              " (3, 20),\n",
              " (27, 3),\n",
              " (27, 2),\n",
              " (27, 20),\n",
              " (27, 10),\n",
              " (20, 27),\n",
              " (20, 3),\n",
              " (20, 10),\n",
              " (20, 32),\n",
              " (10, 20),\n",
              " (10, 27),\n",
              " (10, 32),\n",
              " (10, 25),\n",
              " (32, 10),\n",
              " (32, 20),\n",
              " (32, 25),\n",
              " (32, 8),\n",
              " (25, 32),\n",
              " (25, 10),\n",
              " (25, 8),\n",
              " (25, 10),\n",
              " (8, 25),\n",
              " (8, 32),\n",
              " (8, 10),\n",
              " (8, 15),\n",
              " (10, 8),\n",
              " (10, 25),\n",
              " (10, 15),\n",
              " (10, 31),\n",
              " (15, 10),\n",
              " (15, 8),\n",
              " (15, 31),\n",
              " (15, 6),\n",
              " (31, 15),\n",
              " (31, 10),\n",
              " (31, 6),\n",
              " (31, 28),\n",
              " (6, 31),\n",
              " (6, 15),\n",
              " (6, 28),\n",
              " (6, 13),\n",
              " (28, 6),\n",
              " (28, 31),\n",
              " (28, 13),\n",
              " (28, 28),\n",
              " (13, 28),\n",
              " (13, 6),\n",
              " (13, 28),\n",
              " (13, 1),\n",
              " (28, 13),\n",
              " (28, 28),\n",
              " (28, 1),\n",
              " (28, 19),\n",
              " (1, 28),\n",
              " (1, 13),\n",
              " (1, 19),\n",
              " (1, 28),\n",
              " (19, 1),\n",
              " (19, 28),\n",
              " (19, 28),\n",
              " (19, 13),\n",
              " (28, 19),\n",
              " (28, 1),\n",
              " (28, 13),\n",
              " (28, 28),\n",
              " (13, 28),\n",
              " (13, 19),\n",
              " (13, 28),\n",
              " (13, 12),\n",
              " (28, 13),\n",
              " (28, 28),\n",
              " (28, 12),\n",
              " (28, 28),\n",
              " (12, 28),\n",
              " (12, 13),\n",
              " (12, 28),\n",
              " (12, 14),\n",
              " (28, 12),\n",
              " (28, 28),\n",
              " (28, 14),\n",
              " (28, 37)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# IMPORTS\n",
        "# ---------------------------------------------------------\n",
        "# These classes come from PyTorch ‚Äî they are used to handle\n",
        "# datasets and batching during model training.\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# DEFINE CUSTOM DATASET CLASS\n",
        "# ---------------------------------------------------------\n",
        "# This class wraps your (target, context) training pairs into\n",
        "# a structure compatible with PyTorch's DataLoader.\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class Word2VecDataset(Dataset):  # Inherit from PyTorch's base Dataset class\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        Constructor: called when you create a new dataset object.\n",
        "        'data' is expected to be a list of tuples (target, context).\n",
        "        Example: [(2, 5), (1, 4), (3, 2), ...]\n",
        "        \"\"\"\n",
        "        self.data = data  # Store the input data inside the object\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "        This is required by PyTorch so it knows the dataset size.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a single sample (target, context) pair at the given index.\n",
        "        This is called automatically when the DataLoader fetches a batch.\n",
        "        \"\"\"\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CREATE DATASET AND DATALOADER OBJECTS\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Wrap your training data (list of tuples) inside the custom dataset\n",
        "dataset = Word2VecDataset(training_data)\n",
        "\n",
        "# Use PyTorch's DataLoader to:\n",
        "#   - Automatically divide data into batches\n",
        "#   - Shuffle the data for better training\n",
        "#   - Feed it to the model during each training iteration\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "wWeBUF3s7W5_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KcGi9tvn7Yh4",
        "outputId": "9a63ee97-fb02-4996-b3dc-4391c88460df"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f6d5656a8d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# FUNCTION: get_negative_samples\n",
        "# ---------------------------------------------------------\n",
        "# Purpose:\n",
        "#   Generate random word indices (negative samples)\n",
        "#   that act as \"noise words\" ‚Äî words *not* related to\n",
        "#   the target word in the current training pair.\n",
        "#\n",
        "# Why:\n",
        "#   Instead of updating weights for *every* word in the vocabulary,\n",
        "#   we update only for:\n",
        "#     - The real positive context word\n",
        "#     - A few randomly chosen \"negative\" words\n",
        "#   ‚Üí This makes training thousands of times faster.\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import numpy as np  # NumPy for random number generation\n",
        "\n",
        "def get_negative_samples(target, num_negative_samples, vocab_size):\n",
        "    # Create an empty list to store negative sample indices\n",
        "    neg_samples = []\n",
        "\n",
        "    # Keep generating random indices until we have enough negatives\n",
        "    while len(neg_samples) < num_negative_samples:\n",
        "        # Randomly pick an integer between 0 and (vocab_size - 1)\n",
        "        # Each integer corresponds to a word in the vocabulary.\n",
        "        neg_sample = np.random.randint(0, vocab_size)\n",
        "\n",
        "        # Avoid sampling the target word itself\n",
        "        # because the negative words must be *different* from the true target.\n",
        "        if neg_sample != target:\n",
        "            neg_samples.append(neg_sample)\n",
        "\n",
        "    # Return the list of negative sample word indices\n",
        "    return neg_samples\n"
      ],
      "metadata": {
        "id": "30JQsme28WP3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CLASS: SkipGramNegSampling\n",
        "# ---------------------------------------------------------\n",
        "# Implements the Skip-gram model with Negative Sampling\n",
        "# using two embedding matrices:\n",
        "#   - self.embeddings: for target (center) words\n",
        "#   - self.context_embeddings: for context (neighbor) words\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class SkipGramNegSampling(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGramNegSampling, self).__init__()\n",
        "\n",
        "        # Total number of words in the vocabulary\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Embedding layer for target (center) words\n",
        "        # Each word index maps to a dense vector of length embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Embedding layer for context words\n",
        "        # üí° At the end, the embeddings from these two layers are often averaged or reused as the final word vectors.\n",
        "        # Having separate weights allows different representations\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Log-sigmoid activation function\n",
        "        # log(œÉ(x)) = log(1 / (1 + exp(-x)))\n",
        "        # Used for numerical stability in negative sampling loss\n",
        "        self.log_sigmoid = nn.LogSigmoid()\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # FORWARD METHOD\n",
        "    # ---------------------------------------------------------\n",
        "    # Inputs:\n",
        "    #   target  ‚Üí tensor of word indices for the center words\n",
        "    #   context ‚Üí tensor of word indices for the positive context words\n",
        "    #   negative_samples ‚Üí tensor of indices for sampled noise words\n",
        "    #\n",
        "    # Output:\n",
        "    #   loss value (scalar)\n",
        "    # ---------------------------------------------------------\n",
        "    def forward(self, target, context, negative_samples):\n",
        "\n",
        "        # -----------------------------------------------------\n",
        "        # Step 1: Get embeddings for all inputs\n",
        "        # -----------------------------------------------------\n",
        "\n",
        "        # Embedding lookup for target (center) words\n",
        "        # Shape: [batch_size, embedding_dim]\n",
        "        target_embedding = self.embeddings(target)\n",
        "\n",
        "        # Embedding lookup for context (true positive) words\n",
        "        # Shape: [batch_size, embedding_dim]\n",
        "        context_embedding = self.context_embeddings(context)\n",
        "\n",
        "        # Embedding lookup for negative (noise) words\n",
        "        # Shape: [batch_size, num_neg_samples, embedding_dim]\n",
        "        negative_embeddings = self.context_embeddings(negative_samples)\n",
        "\n",
        "        # -----------------------------------------------------\n",
        "        # Step 2: Compute scores for positive pairs\n",
        "        # -----------------------------------------------------\n",
        "        # Dot product between target and context embeddings:\n",
        "        #   s_pos = target_embedding ¬∑ context_embedding\n",
        "        # Then apply log-sigmoid to get log(œÉ(s_pos))\n",
        "        # Shape: [batch_size]\n",
        "        positive_score = self.log_sigmoid(\n",
        "            torch.sum(target_embedding * context_embedding, dim=1)\n",
        "        )\n",
        "\n",
        "        # -----------------------------------------------------\n",
        "        # Step 3: Compute scores for negative pairs\n",
        "        # -----------------------------------------------------\n",
        "        # Compute dot products between target embeddings and\n",
        "        # each negative context word.\n",
        "        #\n",
        "        #   s_neg = - (target_embedding ¬∑ neg_embedding)\n",
        "        #\n",
        "        # torch.bmm performs batch matrix multiplication:\n",
        "        #   negative_embeddings: [batch, num_neg, embed_dim]\n",
        "        #   target_embedding.unsqueeze(2): [batch, embed_dim, 1]\n",
        "        # Result: [batch, num_neg, 1]\n",
        "        #\n",
        "        # After squeezing and summing across negative samples,\n",
        "        # we get the total negative log probability per batch.\n",
        "        negative_score = self.log_sigmoid(\n",
        "            -torch.bmm(negative_embeddings, target_embedding.unsqueeze(2))\n",
        "            .squeeze(2)\n",
        "        ).sum(1)\n",
        "\n",
        "        # -----------------------------------------------------\n",
        "        # Step 4: Combine and compute final loss\n",
        "        # -----------------------------------------------------\n",
        "        # Loss for each sample:\n",
        "        #   L = - (log œÉ(s_pos) + Œ£ log œÉ(-s_neg))\n",
        "        #\n",
        "        # Then average over the entire batch\n",
        "        loss = - (positive_score + negative_score).mean()\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "PrL8I3rq8WMn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent ‚Äî this is one of the most conceptually deep parts of Word2Vec, so let‚Äôs break this down **line by line**, explaining both **what the code does** and **why it exists mathematically**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Background: What Skip-Gram with Negative Sampling Does\n",
        "\n",
        "Word2Vec‚Äôs **Skip-Gram** model tries to learn **vector representations (embeddings)** of words such that:\n",
        "\n",
        "> Words that appear in similar contexts end up having similar vectors.\n",
        "\n",
        "Instead of predicting *the entire vocabulary* (which is slow), **Negative Sampling** teaches the model to:\n",
        "\n",
        "* **Increase similarity** (dot product) between a real word pair: (target, context)\n",
        "* **Decrease similarity** between fake pairs: (target, random noise words)\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Code Explanation in Detail\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "```\n",
        "\n",
        "* `torch` ‚Üí The main PyTorch library for tensors (multi-dimensional arrays).\n",
        "* `torch.nn` ‚Üí Contains neural network building blocks (like layers, activations, and losses).\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "class SkipGramNegSampling(nn.Module):\n",
        "```\n",
        "\n",
        "This defines a **PyTorch neural network class** for the **Skip-Gram model using Negative Sampling**.\n",
        "\n",
        "By inheriting from `nn.Module`, this class gains:\n",
        "\n",
        "* trainable parameters\n",
        "* automatic differentiation\n",
        "* standard model behavior (`forward`, `state_dict`, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ `__init__` Constructor\n",
        "\n",
        "```python\n",
        "def __init__(self, vocab_size, embedding_dim):\n",
        "    super(SkipGramNegSampling, self).__init__()\n",
        "```\n",
        "\n",
        "* `vocab_size` ‚Üí number of unique words in your vocabulary.\n",
        "* `embedding_dim` ‚Üí how many features each word vector has (e.g. 100 or 300 dimensions).\n",
        "* `super()` initializes the PyTorch base class functionality.\n",
        "\n",
        "---\n",
        "\n",
        "#### 1Ô∏è‚É£ Define model parameters\n",
        "\n",
        "```python\n",
        "self.vocab_size = vocab_size\n",
        "```\n",
        "\n",
        "Stores the total number of words ‚Äî useful for embedding layers.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2Ô∏è‚É£ Embedding layers\n",
        "\n",
        "```python\n",
        "self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "```\n",
        "\n",
        "* Creates a lookup table for **target words (center words)**.\n",
        "  Each word ID (0 to vocab_size‚àí1) maps to a learnable vector.\n",
        "\n",
        "Example:\n",
        "\n",
        "| Word  | ID | Embedding (size 3 example) |\n",
        "| ----- | -- | -------------------------- |\n",
        "| ‚Äúdog‚Äù | 12 | [0.34, -0.17, 0.29]        |\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "```\n",
        "\n",
        "* Creates a *separate* lookup table for **context words** (neighbors around the target).\n",
        "* Having two matrices allows the model to learn:\n",
        "\n",
        "  * `W_target` ‚Üí what a word *is*\n",
        "  * `W_context` ‚Üí how a word *is used around others*\n",
        "\n",
        "üí° At the end, the embeddings from these two layers are often averaged or reused as the final word vectors.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3Ô∏è‚É£ Activation function\n",
        "\n",
        "```python\n",
        "self.log_sigmoid = nn.LogSigmoid()\n",
        "```\n",
        "\n",
        "* Computes:\n",
        "  [\n",
        "  \\log(\\sigma(x)) = \\log\\left(\\frac{1}{1+e^{-x}}\\right)\n",
        "  ]\n",
        "* Used because it‚Äôs **numerically stable** compared to using `torch.log(torch.sigmoid(x))`.\n",
        "* The skip-gram negative sampling loss formula involves:\n",
        "\n",
        "  * `log(sigmoid(positive_score))`\n",
        "  * `log(sigmoid(-negative_score))`\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ `forward` Method ‚Äî Main Logic\n",
        "\n",
        "```python\n",
        "def forward(self, target, context, negative_samples):\n",
        "```\n",
        "\n",
        "* `target`: Tensor of word IDs for center words. Shape: `[batch_size]`\n",
        "* `context`: Tensor of word IDs for **true context** words. Shape: `[batch_size]`\n",
        "* `negative_samples`: Tensor of word IDs for **noise words**. Shape: `[batch_size, num_negatives]`\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 1Ô∏è‚É£: Embedding Lookups\n",
        "\n",
        "```python\n",
        "target_embedding = self.embeddings(target)\n",
        "context_embedding = self.context_embeddings(context)\n",
        "negative_embeddings = self.context_embeddings(negative_samples)\n",
        "```\n",
        "\n",
        "Shapes:\n",
        "\n",
        "| Variable              | Shape                                  | Meaning                                   |\n",
        "| --------------------- | -------------------------------------- | ----------------------------------------- |\n",
        "| `target_embedding`    | `[batch_size, embedding_dim]`          | vector for each center word               |\n",
        "| `context_embedding`   | `[batch_size, embedding_dim]`          | vector for each true context word         |\n",
        "| `negative_embeddings` | `[batch_size, num_neg, embedding_dim]` | vectors for randomly chosen \"wrong\" words |\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 2Ô∏è‚É£: Positive Pair Score\n",
        "\n",
        "```python\n",
        "positive_score = self.log_sigmoid(\n",
        "    torch.sum(target_embedding * context_embedding, dim=1)\n",
        ")\n",
        "```\n",
        "\n",
        "Breakdown:\n",
        "\n",
        "1. `target_embedding * context_embedding` ‚Üí elementwise multiply vectors.\n",
        "2. `torch.sum(..., dim=1)` ‚Üí compute dot product for each pair (center‚Äìcontext).\n",
        "   [\n",
        "   s_{pos} = v_{target} \\cdot v_{context}\n",
        "   ]\n",
        "3. `log_sigmoid(...)` ‚Üí apply log(œÉ(s‚Çö‚Çí‚Çõ)), which rewards large positive dot products (i.e., similar vectors).\n",
        "\n",
        "Intuition:\n",
        "\n",
        "> The model is learning to **increase similarity** between words that actually co-occur.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 3Ô∏è‚É£: Negative Pair Scores\n",
        "\n",
        "```python\n",
        "negative_score = self.log_sigmoid(\n",
        "    -torch.bmm(negative_embeddings, target_embedding.unsqueeze(2)).squeeze(2)\n",
        ").sum(1)\n",
        "```\n",
        "\n",
        "Let‚Äôs decode that monster step by step üëá\n",
        "\n",
        "1. `target_embedding.unsqueeze(2)`\n",
        "   Adds a dimension ‚Üí `[batch, embed_dim, 1]`\n",
        "   Needed to perform matrix multiplication.\n",
        "\n",
        "2. `torch.bmm(negative_embeddings, target_embedding.unsqueeze(2))`\n",
        "   Batch matrix multiply:\n",
        "\n",
        "   * `negative_embeddings`: `[batch, num_neg, embed_dim]`\n",
        "   * `target_embedding.unsqueeze(2)`: `[batch, embed_dim, 1]`\n",
        "   * Output: `[batch, num_neg, 1]`\n",
        "     ‚Üí Each element = dot product between target and one negative sample.\n",
        "\n",
        "3. `- (...)`\n",
        "   Negates the scores (because we want `log œÉ(-s_neg)`).\n",
        "\n",
        "4. `.squeeze(2)`\n",
        "   Removes the last dimension ‚Üí `[batch, num_neg]`\n",
        "\n",
        "5. `self.log_sigmoid(...)`\n",
        "   Computes `log œÉ(-score)` ‚Üí this punishes similarity with wrong words.\n",
        "\n",
        "6. `.sum(1)`\n",
        "   Sums across all negative samples per target.\n",
        "\n",
        "Intuition:\n",
        "\n",
        "> The model **reduces similarity** between the target word and random noise words.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 4Ô∏è‚É£: Combine Positive + Negative Loss\n",
        "\n",
        "```python\n",
        "loss = - (positive_score + negative_score).mean()\n",
        "```\n",
        "\n",
        "Mathematical formula:\n",
        "[\n",
        "L = -\\frac{1}{N} \\sum_{i=1}^{N} [ \\log \\sigma(v_{t_i} \\cdot v_{c_i}) + \\sum_{k=1}^{K} \\log \\sigma(-v_{t_i} \\cdot v_{n_{ik}}) ]\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* ( v_{t_i} ): target word vector\n",
        "* ( v_{c_i} ): context word vector\n",
        "* ( v_{n_{ik}} ): k-th negative sample vector\n",
        "* ( K ): number of negative samples\n",
        "\n",
        "This loss encourages:\n",
        "\n",
        "* Large dot product for positive pairs\n",
        "* Small (negative) dot product for negative pairs\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Concept Summary\n",
        "\n",
        "| Concept               | Meaning                                                  |\n",
        "| --------------------- | -------------------------------------------------------- |\n",
        "| **Embedding layers**  | Store and learn vector representations of words          |\n",
        "| **Positive sampling** | Pairs of real co-occurring words (target‚Äìcontext)        |\n",
        "| **Negative sampling** | Pairs of (target‚Äìrandom words) that *shouldn‚Äôt* co-occur |\n",
        "| **Dot product**       | Measures similarity between two vectors                  |\n",
        "| **Log-sigmoid**       | Converts similarity into probability-like scores         |\n",
        "| **Loss**              | Maximizes log(œÉ(pos)) and minimizes log(œÉ(-neg))         |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "xQfwmjLIVh-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# TRAINING THE SKIP-GRAM MODEL WITH NEGATIVE SAMPLING\n",
        "# ---------------------------------------------\n",
        "\n",
        "# Total number of unique words in the vocabulary\n",
        "vocab_size = len(word_to_idx)\n",
        "\n",
        "# Initialize the Skip-gram model with given vocabulary size and embedding dimension\n",
        "model = SkipGramNegSampling(vocab_size, embedding_dim)\n",
        "\n",
        "# Define the optimizer (Adam) to update the model parameters (embeddings)\n",
        "# learning_rate controls how big the update steps are\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# MAIN TRAINING LOOP\n",
        "# ---------------------------------------------\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0  # To accumulate total loss for each epoch\n",
        "\n",
        "    # Loop over all mini-batches of (target, context) pairs\n",
        "    for target, context in dataloader:\n",
        "\n",
        "        # Ensure that both tensors are of type long (required by nn.Embedding)\n",
        "        target = target.long()\n",
        "        context = context.long()\n",
        "\n",
        "        # -------------------------------------------------\n",
        "        # NEGATIVE SAMPLING\n",
        "        # -------------------------------------------------\n",
        "        # For each target word in the batch, randomly pick 'num_negative_samples'\n",
        "        # words from the vocabulary that are *not* the target.\n",
        "        # These are \"fake\" context words (noise samples).\n",
        "        negative_samples = torch.LongTensor([\n",
        "            get_negative_samples(t.item(), num_negative_samples, vocab_size)\n",
        "            for t in target\n",
        "        ])\n",
        "\n",
        "        # -------------------------------------------------\n",
        "        # FORWARD AND BACKWARD PASSES\n",
        "        # -------------------------------------------------\n",
        "\n",
        "        # 1. Clear any previously stored gradients before backpropagation\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Compute the model loss for this batch\n",
        "        # The model internally:\n",
        "        #   - Looks up embeddings for targets, contexts, and negatives\n",
        "        #   - Computes positive and negative log-sigmoid scores\n",
        "        #   - Returns average loss for the batch\n",
        "        loss = model(target, context, negative_samples)\n",
        "\n",
        "        # 3. Compute gradients (‚àÇLoss/‚àÇParameters)\n",
        "        # This step performs automatic differentiation\n",
        "        loss.backward()\n",
        "\n",
        "        # 4. Update embeddings using gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # -------------------------------------------------\n",
        "        # TRACKING PROGRESS\n",
        "        # -------------------------------------------------\n",
        "\n",
        "        # Add current batch loss to the total loss for this epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # DISPLAY AVERAGE EPOCH LOSS\n",
        "    # -------------------------------------------------\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "w9B20Lg08WJf",
        "outputId": "b0b3caca-d399-4a39-ac73-88bcdf7daf11"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 23.6487\n",
            "Epoch 2, Loss: 23.7863\n",
            "Epoch 3, Loss: 23.3079\n",
            "Epoch 4, Loss: 22.4350\n",
            "Epoch 5, Loss: 23.8155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# GETTING THE LEARNED WORD EMBEDDINGS\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# Extract the learned embeddings from the model\n",
        "# model.embeddings.weight ‚Üí tensor containing learned word vectors for all words\n",
        "# detach() ‚Üí disconnects from computation graph (no gradients needed now)\n",
        "# numpy() ‚Üí converts PyTorch tensor into a NumPy array for easier math operations\n",
        "embeddings = model.embeddings.weight.detach().numpy()\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# FUNCTION: FIND SIMILAR WORDS USING COSINE SIMILARITY\n",
        "# -----------------------------------------------------\n",
        "\n",
        "def get_similar_words(word, top_n=5):\n",
        "    \"\"\"\n",
        "    Given a word, find the top-N most similar words based on cosine similarity.\n",
        "    \"\"\"\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Step 1: Get the embedding vector for the given word\n",
        "    # -------------------------------------------------\n",
        "    idx = word_to_idx[word]            # Get the word's numeric ID\n",
        "    word_embedding = embeddings[idx]   # Retrieve its vector representation\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Step 2: Compute similarity scores with all other words\n",
        "    # -------------------------------------------------\n",
        "    # np.dot(embeddings, word_embedding) computes the dot product\n",
        "    # between the target word and every other word in the vocabulary.\n",
        "    # This is equivalent to:\n",
        "    #   similarity(i) = v_i ¬∑ v_target\n",
        "    #\n",
        "    # Note: For true cosine similarity, you‚Äôd usually normalize vectors,\n",
        "    # but since Word2Vec vectors tend to be roughly normalized already,\n",
        "    # this approximation often works fine.\n",
        "    similarities = np.dot(embeddings, word_embedding)\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Step 3: Sort words by similarity (highest first)\n",
        "    # -------------------------------------------------\n",
        "    # argsort() returns indices sorted in ascending order.\n",
        "    # By adding a negative sign (-similarities), we sort in descending order.\n",
        "    # [1:top_n+1] skips the first element, which is the word itself.\n",
        "    closest_idxs = (-similarities).argsort()[1:top_n + 1]\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Step 4: Convert indices back to readable words\n",
        "    # -------------------------------------------------\n",
        "    # idx_to_word maps numeric indices back to word strings\n",
        "    return [idx_to_word[idx] for idx in closest_idxs]\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# EXAMPLE USAGE\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# Print top 5 words most similar to \"do\"\n",
        "print(get_similar_words(\"do\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xHagvmdb8WGX",
        "outputId": "f28b7a91-8b3c-4725-f971-3671baf8b2fa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['excellence', 'think', 'strike', 'repeatedly', 'can']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B95ISZzb8WDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TZ42tPE38V9n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}