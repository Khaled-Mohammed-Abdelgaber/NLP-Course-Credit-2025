{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ef16fa",
   "metadata": {},
   "source": [
    "# Word2Vec: A Complete Deep Dive from Scratch\n",
    "\n",
    "Let me build this up systematically, explaining every concept, algorithm, and design decision.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: The Motivation - Why Word2Vec?\n",
    "\n",
    "### The Limitations of Previous Approaches\n",
    "\n",
    "**Recall LSA/SVD:**\n",
    "- Operates on entire corpus at once (global statistics)\n",
    "- Expensive to update (must recompute SVD)\n",
    "- Requires huge matrix factorization\n",
    "- Linear algebra operations scale poorly\n",
    "\n",
    "**One-hot encoding:**\n",
    "```\n",
    "cat  = [1, 0, 0, 0, 0, ...]\n",
    "dog  = [0, 1, 0, 0, 0, ...]\n",
    "meow = [0, 0, 1, 0, 0, ...]\n",
    "```\n",
    "\n",
    "Problems:\n",
    "- Every word is equally distant from every other word\n",
    "- No semantic information whatsoever\n",
    "- Vocabulary-sized vectors (50,000+ dimensions)\n",
    "\n",
    "### The Word2Vec Revolution (2013)\n",
    "\n",
    "**Key insight by Tomas Mikolov et al.**: What if we predict words from their context (or context from words) using a neural network? The network's internal representations would naturally capture semantic meaning!\n",
    "\n",
    "**Core philosophy**: \"You shall know a word by the company it keeps\" (Firth, 1957)\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: The Fundamental Idea\n",
    "\n",
    "### Dense Word Embeddings\n",
    "\n",
    "Word2Vec learns to represent each word as a **dense vector** (typically 100-300 dimensions):\n",
    "\n",
    "```\n",
    "cat  = [0.2, -0.4, 0.7, 0.1, ..., 0.3]  # 300 numbers\n",
    "dog  = [0.3, -0.3, 0.6, 0.2, ..., 0.4]  # 300 numbers\n",
    "meow = [0.1, -0.5, 0.8, 0.0, ..., 0.2]  # 300 numbers\n",
    "```\n",
    "\n",
    "**Why this is better:**\n",
    "1. **Dense**: Every dimension has a value (no sparsity)\n",
    "2. **Low-dimensional**: 300 << 50,000\n",
    "3. **Semantic**: Similar words have similar vectors\n",
    "4. **Algebraic**: Vector arithmetic captures relationships\n",
    "\n",
    "### The Distributional Hypothesis in Action\n",
    "\n",
    "Words appearing in similar contexts should have similar embeddings:\n",
    "\n",
    "```\n",
    "\"The ___ is sleeping on the mat\"\n",
    "```\n",
    "\n",
    "Could be: cat, dog, hamster, rabbit → all get similar embeddings\n",
    "\n",
    "```\n",
    "\"The ___ flew over the building\"\n",
    "```\n",
    "\n",
    "Could be: bird, plane, helicopter → different cluster of similar embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Two Architectures\n",
    "\n",
    "Word2Vec has two main architectures. Let me explain both thoroughly.\n",
    "\n",
    "## Architecture 1: CBOW (Continuous Bag of Words)\n",
    "\n",
    "### The Core Task\n",
    "\n",
    "**Given context words, predict the center word**\n",
    "\n",
    "Example sentence: \"The quick brown fox jumps\"\n",
    "- Context: [\"The\", \"quick\", \"brown\", \"jumps\"]\n",
    "- Target: \"fox\"\n",
    "\n",
    "### Why \"Bag of Words\"?\n",
    "\n",
    "The order of context words doesn't matter - we just take their average. \"Quick brown\" = \"brown quick\" for CBOW.\n",
    "\n",
    "**Justification**: Simplifies the model and makes it faster. Word order information is sacrificed for computational efficiency.\n",
    "\n",
    "### The Architecture (Detailed)\n",
    "\n",
    "Let me walk through the exact neural network structure:\n",
    "\n",
    "#### **Components:**\n",
    "\n",
    "1. **Input layer**: One-hot encoded context words\n",
    "   - Vocabulary size V (e.g., 50,000)\n",
    "   \n",
    "2. **Hidden layer** (the embedding layer): \n",
    "   - Size N (e.g., 300 dimensions)\n",
    "   - No activation function! (linear)\n",
    "   - This is where embeddings live\n",
    "   \n",
    "3. **Output layer**: Probability distribution over vocabulary\n",
    "   - Size V\n",
    "   - Softmax activation\n",
    "\n",
    "#### **Weight Matrices:**\n",
    "\n",
    "- **W_input** (V × N): Input-to-hidden weights\n",
    "  - Each row is the embedding vector for a word\n",
    "  - This is the **word embedding matrix** we want to learn!\n",
    "  \n",
    "- **W_output** (N × V): Hidden-to-output weights\n",
    "  - Each column represents a word in the output space\n",
    "  - Often called the \"context matrix\"\n",
    "\n",
    "### Forward Pass (Step by Step)\n",
    "\n",
    "**Given**: Context words w₁, w₂, ..., wc (c = context size)\n",
    "\n",
    "**Step 1**: Convert each context word to one-hot vector\n",
    "```\n",
    "w₁ → x₁ = [0, 0, ..., 1, ..., 0]  (1 at position of w₁)\n",
    "w₂ → x₂ = [0, 0, ..., 1, ..., 0]  (1 at position of w₂)\n",
    "```\n",
    "\n",
    "**Step 2**: Look up embedding for each context word\n",
    "```\n",
    "v₁ = W_input[w₁]  (just extract row w₁ from W_input)\n",
    "v₂ = W_input[w₂]\n",
    "...\n",
    "vc = W_input[wc]\n",
    "```\n",
    "\n",
    "**Step 3**: Average the context embeddings\n",
    "```\n",
    "h = (v₁ + v₂ + ... + vc) / c\n",
    "```\n",
    "\n",
    "**Justification for averaging**: Equal weight to all context words. Simple and works well. Creates a \"center of mass\" in embedding space.\n",
    "\n",
    "**Step 4**: Compute output scores\n",
    "```\n",
    "u = W_output^T · h\n",
    "```\n",
    "Result: u is a V-dimensional vector (one score per vocabulary word)\n",
    "\n",
    "**Step 5**: Apply softmax to get probabilities\n",
    "```\n",
    "p(word_i | context) = exp(u_i) / Σⱼ exp(u_j)\n",
    "```\n",
    "\n",
    "**Justification for softmax**: Converts scores to valid probability distribution (sums to 1, all positive).\n",
    "\n",
    "### The Learning Objective\n",
    "\n",
    "**Goal**: Maximize the probability of predicting the correct center word\n",
    "\n",
    "**Loss function** (negative log-likelihood):\n",
    "```\n",
    "L = -log p(w_center | context)\n",
    "  = -u_center + log(Σⱼ exp(u_j))\n",
    "```\n",
    "\n",
    "We want to:\n",
    "- Increase score for correct word (u_center)\n",
    "- Decrease scores for incorrect words (denominator)\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "**The gradient flow**:\n",
    "\n",
    "1. Output layer gradient:\n",
    "```\n",
    "∂L/∂u_i = p(word_i | context) - 1[i = center]\n",
    "```\n",
    "Where 1[...] is 1 if true, 0 otherwise.\n",
    "\n",
    "**Interpretation**: Error = predicted probability - true probability (1 for correct word, 0 for others)\n",
    "\n",
    "2. Update W_output:\n",
    "```\n",
    "∂L/∂W_output = h · (p - t)^T\n",
    "```\n",
    "Where p = predicted probabilities, t = target (one-hot)\n",
    "\n",
    "3. Hidden layer gradient:\n",
    "```\n",
    "∂L/∂h = W_output · (p - t)\n",
    "```\n",
    "\n",
    "4. Update W_input (the embeddings!):\n",
    "```\n",
    "∂L/∂W_input[wᵢ] = (∂L/∂h) / c  for each context word wᵢ\n",
    "```\n",
    "\n",
    "**Key insight**: Only the embeddings of words that appeared in the context get updated! This is efficient.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture 2: Skip-gram\n",
    "\n",
    "### The Core Task\n",
    "\n",
    "**Given center word, predict context words**\n",
    "\n",
    "This is the **reverse** of CBOW!\n",
    "\n",
    "Example sentence: \"The quick brown fox jumps\"\n",
    "- Input: \"fox\"\n",
    "- Targets: [\"The\", \"quick\", \"brown\", \"jumps\"]\n",
    "\n",
    "### Why Skip-gram?\n",
    "\n",
    "**Justification**: Skip-gram works better for:\n",
    "- Rare words (gets more training examples per occurrence)\n",
    "- Smaller datasets\n",
    "- Learning more precise embeddings\n",
    "\n",
    "CBOW works better for:\n",
    "- Frequent words\n",
    "- Larger datasets\n",
    "- Faster training\n",
    "\n",
    "### The Architecture\n",
    "\n",
    "**Components:**\n",
    "\n",
    "1. **Input layer**: One-hot encoding of center word (V dimensions)\n",
    "2. **Hidden layer**: Embedding layer (N dimensions, no activation)\n",
    "3. **Output layer**: C separate softmax layers (one per context position)\n",
    "\n",
    "Or more commonly: **Single output layer** that predicts one context word at a time.\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "**Given**: Center word w_center\n",
    "\n",
    "**Step 1**: Get embedding\n",
    "```\n",
    "h = W_input[w_center]\n",
    "```\n",
    "\n",
    "**Step 2**: Compute output scores for each context position\n",
    "```\n",
    "For each position i in context:\n",
    "    u^(i) = W_output^T · h\n",
    "    p(w | w_center) = softmax(u^(i))\n",
    "```\n",
    "\n",
    "### The Learning Objective\n",
    "\n",
    "**Goal**: Maximize probability of predicting all context words\n",
    "\n",
    "**Loss function**:\n",
    "```\n",
    "L = -Σᵢ log p(w_context_i | w_center)\n",
    "```\n",
    "\n",
    "We process multiple context words, so we sum their losses.\n",
    "\n",
    "### Skip-gram with Multiple Contexts\n",
    "\n",
    "For sentence: \"The quick brown fox jumps over\"\n",
    "Center word: \"fox\"\n",
    "Window size: 2\n",
    "\n",
    "Training pairs generated:\n",
    "- (fox, The)\n",
    "- (fox, quick)\n",
    "- (fox, brown)\n",
    "- (fox, jumps)\n",
    "\n",
    "**Each pair is a separate training example!**\n",
    "\n",
    "**Why this helps rare words**: If \"fox\" appears once, we get 4 training examples from that single occurrence. CBOW would give us only 1.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: The Computational Problem\n",
    "\n",
    "### The Softmax Bottleneck\n",
    "\n",
    "The softmax denominator is the killer:\n",
    "\n",
    "```\n",
    "p(w | context) = exp(u_w) / Σⱼ₌₁^V exp(u_j)\n",
    "```\n",
    "\n",
    "We must sum over the **entire vocabulary** (V = 50,000+) for every prediction!\n",
    "\n",
    "**Computational cost**: O(V) per training example\n",
    "**With millions of training examples**: Completely impractical!\n",
    "\n",
    "This is why we need approximations.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Solution 1 - Hierarchical Softmax\n",
    "\n",
    "### The Idea\n",
    "\n",
    "Instead of computing probability over V words directly, organize vocabulary as a **binary tree**.\n",
    "\n",
    "**Key insight**: Any word can be reached by a path of log₂(V) binary decisions.\n",
    "\n",
    "### The Tree Structure\n",
    "\n",
    "```\n",
    "                    root\n",
    "                   /    \\\n",
    "              left/      \\right\n",
    "                /          \\\n",
    "           node1           node2\n",
    "           /   \\           /   \\\n",
    "        word1 word2    word3  word4\n",
    "```\n",
    "\n",
    "**Huffman tree**: More frequent words get shorter paths (fewer decisions).\n",
    "\n",
    "**Justification**: Frequent words are predicted more often, so making them faster improves overall speed.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Instead of V-way classification, we make log₂(V) binary classifications.\n",
    "\n",
    "**Each node in the tree has a vector** (same dimension as embeddings).\n",
    "\n",
    "**Probability of a word**:\n",
    "```\n",
    "p(w | context) = Πᵢ₌₁^L p(decision_i | context)\n",
    "```\n",
    "\n",
    "Where L = length of path to word w\n",
    "\n",
    "**Binary decision at node n**:\n",
    "```\n",
    "p(left | h, n) = σ(v_n^T · h)\n",
    "p(right | h, n) = 1 - σ(v_n^T · h)\n",
    "```\n",
    "\n",
    "Where σ = sigmoid function, v_n = vector for node n\n",
    "\n",
    "### Computational Savings\n",
    "\n",
    "- **Before**: O(V) operations per prediction\n",
    "- **After**: O(log V) operations per prediction\n",
    "\n",
    "For V = 50,000: 50,000 → 16 operations!\n",
    "\n",
    "**Trade-off**: Slightly more complex implementation, but massive speedup.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 6: Solution 2 - Negative Sampling (Most Popular!)\n",
    "\n",
    "### The Brilliant Insight\n",
    "\n",
    "**Question**: Do we really need to distinguish the target word from ALL other words?\n",
    "\n",
    "**Answer**: No! Just distinguish it from a **small sample** of \"negative\" words.\n",
    "\n",
    "### The Objective\n",
    "\n",
    "Original softmax objective:\n",
    "```\n",
    "maximize: log p(w_center | context)\n",
    "```\n",
    "\n",
    "Negative sampling objective:\n",
    "```\n",
    "maximize: log σ(u_center) + Σₖ₌₁^K log σ(-u_k)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- u_center = score for correct word\n",
    "- u_k = score for k randomly sampled \"negative\" words\n",
    "- σ = sigmoid function\n",
    "- K = number of negative samples (typically 5-20)\n",
    "\n",
    "### What Does This Mean?\n",
    "\n",
    "We're training the model to:\n",
    "1. **Give high score** to the actual context word (log σ(u_center))\n",
    "2. **Give low score** to random words that didn't appear (log σ(-u_k))\n",
    "\n",
    "**Binary classification**: \"Is this word in the context?\" Yes for 1 word, No for K words.\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "**Statistical justification**: If we sample negatives according to word frequency, this approximates the full softmax in expectation.\n",
    "\n",
    "**Intuitive justification**: The model learns to distinguish real word pairs from noise. If it can do this well, it has learned meaningful embeddings!\n",
    "\n",
    "### Negative Sampling Distribution\n",
    "\n",
    "**How to sample negative words?**\n",
    "\n",
    "Original paper uses: P(w) ∝ freq(w)^(3/4)\n",
    "\n",
    "**Why the 3/4 power?**\n",
    "\n",
    "1. **freq(w)^1**: Proportional to frequency\n",
    "   - Problem: Very common words (like \"the\") sampled too often\n",
    "   \n",
    "2. **freq(w)^0**: Uniform sampling\n",
    "   - Problem: Rare words sampled as often as common words (unnatural)\n",
    "   \n",
    "3. **freq(w)^(3/4)**: Sweet spot!\n",
    "   - Rare words sampled more than their frequency\n",
    "   - Common words sampled less than their frequency\n",
    "   - Balances the distribution\n",
    "\n",
    "**Mathematical effect**: \n",
    "- Word with frequency 100 → relative probability ≈ 31.6\n",
    "- Word with frequency 10 → relative probability ≈ 5.6\n",
    "- Ratio: 31.6/5.6 ≈ 5.6 (instead of 10)\n",
    "\n",
    "More balanced!\n",
    "\n",
    "### Computational Cost\n",
    "\n",
    "- **Full softmax**: O(V) operations\n",
    "- **Negative sampling**: O(K) operations\n",
    "\n",
    "For K = 10 and V = 50,000: **5,000× speedup!**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 7: Training Details\n",
    "\n",
    "### Context Window\n",
    "\n",
    "**Window size**: How many words on each side to consider as context?\n",
    "\n",
    "Example with window = 2:\n",
    "```\n",
    "\"The quick brown fox jumps over the lazy dog\"\n",
    "                 ↑\n",
    "            Center: fox\n",
    "Context: [quick, brown, jumps, over]\n",
    "```\n",
    "\n",
    "**Common values**: 5-10 words\n",
    "\n",
    "**Dynamic windows**: Randomly sample window size from 1 to max for each center word.\n",
    "\n",
    "**Justification**: \n",
    "- Nearby words more important (syntax, strong relationships)\n",
    "- Distant words provide semantic information\n",
    "- Dynamic windows give more weight to closer words naturally\n",
    "\n",
    "### Subsampling Frequent Words\n",
    "\n",
    "Very common words (the, a, is) appear too often and provide less information.\n",
    "\n",
    "**Subsampling formula**: Discard word w with probability:\n",
    "\n",
    "```\n",
    "P(discard w) = 1 - √(t / freq(w))\n",
    "```\n",
    "\n",
    "Where t is a threshold (typically 10^-5), freq(w) is word frequency.\n",
    "\n",
    "**Example**:\n",
    "- \"the\" (freq = 0.1): P(discard) ≈ 0.97 (discard 97% of time!)\n",
    "- \"fox\" (freq = 0.0001): P(discard) ≈ 0 (keep it)\n",
    "\n",
    "**Justification**:\n",
    "- Speeds up training significantly\n",
    "- Improves embeddings (less dominated by common words)\n",
    "- Rare words get relatively more attention\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "Start high (0.025), decay linearly to minimum (0.0001) over training.\n",
    "\n",
    "**Justification**: Early - make big moves to find good regions. Late - fine-tune with small adjustments.\n",
    "\n",
    "### Epochs\n",
    "\n",
    "Typically 5-15 passes through the corpus.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 8: The Magic - Semantic Relationships\n",
    "\n",
    "### Vector Arithmetic\n",
    "\n",
    "The famous property of Word2Vec:\n",
    "\n",
    "```\n",
    "king - man + woman ≈ queen\n",
    "paris - france + germany ≈ berlin\n",
    "walking - walk + swim ≈ swimming\n",
    "```\n",
    "\n",
    "### Why Does This Work?\n",
    "\n",
    "**Geometric interpretation**: The model learns that certain **directions** in embedding space represent relationships:\n",
    "\n",
    "- Gender direction: king → queen (same as man → woman)\n",
    "- Country-capital direction: france → paris (same as germany → berlin)\n",
    "- Verb tense direction: walk → walking (same as swim → swimming)\n",
    "\n",
    "**Mathematical explanation**:\n",
    "\n",
    "Words that appear in similar contexts get similar embeddings. But more specifically:\n",
    "\n",
    "- \"king\" appears with: [royal, kingdom, throne, male, ...]\n",
    "- \"queen\" appears with: [royal, kingdom, throne, female, ...]\n",
    "- \"man\" appears with: [male, person, ...]\n",
    "- \"woman\" appears with: [female, person, ...]\n",
    "\n",
    "The difference (king - man) captures \"royalty\" after removing \"male-person.\"\n",
    "Adding \"woman\" brings back \"female-person.\"\n",
    "Result: queen!\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "To find similar words:\n",
    "\n",
    "```\n",
    "similarity(v₁, v₂) = (v₁ · v₂) / (||v₁|| ||v₂||)\n",
    "```\n",
    "\n",
    "**Why cosine?**\n",
    "- Ignores magnitude (long vs short vectors)\n",
    "- Focuses on direction (meaning)\n",
    "- Range: [-1, 1] where 1 = identical direction\n",
    "\n",
    "---\n",
    "\n",
    "## Part 9: Implementation Walkthrough\n",
    "\n",
    "Let me show you a simplified implementation to make everything concrete.\n",
    "\n",
    "### Pseudocode for Skip-gram with Negative Sampling\n",
    "\n",
    "```python\n",
    "# Initialize\n",
    "W_input = random_matrix(V, N)  # V = vocab size, N = embedding dim\n",
    "W_output = random_matrix(V, N)\n",
    "\n",
    "for epoch in epochs:\n",
    "    for sentence in corpus:\n",
    "        for position, center_word in enumerate(sentence):\n",
    "            \n",
    "            # Get context words\n",
    "            window = random_int(1, max_window)\n",
    "            context_words = sentence[position-window : position+window]\n",
    "            context_words.remove(center_word)\n",
    "            \n",
    "            # Subsampling (skip frequent words sometimes)\n",
    "            if random() < discard_probability(center_word):\n",
    "                continue\n",
    "            \n",
    "            # Get center word embedding\n",
    "            v_center = W_input[center_word]  # Shape: (N,)\n",
    "            \n",
    "            for context_word in context_words:\n",
    "                \n",
    "                # Positive example\n",
    "                v_context = W_output[context_word]\n",
    "                score_pos = dot(v_center, v_context)\n",
    "                loss = -log(sigmoid(score_pos))\n",
    "                \n",
    "                # Gradient for positive\n",
    "                grad = (sigmoid(score_pos) - 1)\n",
    "                W_input[center_word] -= lr * grad * v_context\n",
    "                W_output[context_word] -= lr * grad * v_center\n",
    "                \n",
    "                # Negative examples\n",
    "                neg_words = sample_negatives(K, vocabulary)\n",
    "                for neg_word in neg_words:\n",
    "                    v_neg = W_output[neg_word]\n",
    "                    score_neg = dot(v_center, v_neg)\n",
    "                    loss += -log(sigmoid(-score_neg))\n",
    "                    \n",
    "                    # Gradient for negative\n",
    "                    grad = sigmoid(score_neg)\n",
    "                    W_input[center_word] -= lr * grad * v_neg\n",
    "                    W_output[neg_word] -= lr * grad * v_center\n",
    "```\n",
    "\n",
    "### Key Implementation Details\n",
    "\n",
    "1. **Which matrix to use as final embeddings?**\n",
    "   - W_input: Most common choice\n",
    "   - Average of W_input and W_output: Sometimes better\n",
    "   - Concatenate both: Doubles dimensions but captures more\n",
    "\n",
    "2. **Vocabulary preprocessing**:\n",
    "   - Remove very rare words (< 5 occurrences)\n",
    "   - Replace rare words with <UNK> token\n",
    "   - Add special tokens: <PAD>, <START>, <END>\n",
    "\n",
    "3. **Initialization**:\n",
    "   - Random uniform: [-0.5/N, 0.5/N]\n",
    "   - Small random values prevent saturation\n",
    "\n",
    "---\n",
    "\n",
    "## Part 10: Advantages and Limitations\n",
    "\n",
    "### Advantages of Word2Vec\n",
    "\n",
    "1. **Speed**: Much faster than LSA (local context, not global matrix)\n",
    "2. **Incremental**: Can update with new text without retraining from scratch\n",
    "3. **Semantic quality**: Captures analogies and relationships\n",
    "4. **Dense representations**: No sparsity, efficient storage\n",
    "5. **Transfer learning**: Pre-trained embeddings work well for many tasks\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Fixed vocabulary**: New words (out-of-vocabulary) have no embeddings\n",
    "   - **Solution**: Use subword embeddings (FastText)\n",
    "\n",
    "2. **Single embedding per word**: Polysemy problem\n",
    "   - \"bank\" (financial) vs \"bank\" (river) → same embedding!\n",
    "   - **Solution**: Contextual embeddings (ELMo, BERT)\n",
    "\n",
    "3. **No word order**: Bag of context words\n",
    "   - \"not good\" vs \"good\" might be too similar\n",
    "   - **Solution**: Use models that respect order (LSTM, Transformer)\n",
    "\n",
    "4. **Short context**: Only nearby words matter\n",
    "   - Long-range dependencies missed\n",
    "   - **Solution**: Larger windows, or document-level models\n",
    "\n",
    "5. **Training instability**: Sensitive to hyperparameters\n",
    "   - Requires tuning\n",
    "\n",
    "6. **Black box**: Why does this specific embedding make sense? Hard to explain\n",
    "\n",
    "---\n",
    "\n",
    "## Part 11: Variants and Extensions\n",
    "\n",
    "### FastText (2017)\n",
    "\n",
    "**Key innovation**: Represent words as **bag of character n-grams**\n",
    "\n",
    "Example: \"where\"\n",
    "- Character trigrams: <wh, whe, her, ere, re>\n",
    "- Word embedding = sum of n-gram embeddings\n",
    "\n",
    "**Advantages**:\n",
    "- Handles rare words better\n",
    "- Handles misspellings\n",
    "- Generates embeddings for unseen words!\n",
    "- Captures morphology (walk/walking/walked related)\n",
    "\n",
    "### GloVe (2014)\n",
    "\n",
    "**Key insight**: Combine global statistics (like LSA) with local context (like Word2Vec)\n",
    "\n",
    "**Method**: \n",
    "- Build word co-occurrence matrix (global)\n",
    "- Factorize it with a weighted least squares objective\n",
    "- Goal: dot product of embeddings = log of co-occurrence probability\n",
    "\n",
    "**Advantage**: Often better performance, especially on analogy tasks\n",
    "\n",
    "### ELMo (2018)\n",
    "\n",
    "**Revolution**: **Contextual** embeddings\n",
    "\n",
    "Each word gets different embedding based on sentence:\n",
    "- \"I went to the **bank** to deposit money\" → financial embedding\n",
    "- \"I sat by the river **bank**\" → geographical embedding\n",
    "\n",
    "**Method**: Deep bidirectional LSTM language model\n",
    "\n",
    "### BERT (2018)\n",
    "\n",
    "**Even bigger revolution**: Transformer-based contextual embeddings\n",
    "\n",
    "- Uses attention mechanism (not recurrence)\n",
    "- Bidirectional context\n",
    "- Pre-trained on massive corpora\n",
    "- Fine-tuned for specific tasks\n",
    "\n",
    "**Word2Vec's legacy**: It pioneered the idea that embeddings should be learned from data, not hand-crafted!\n",
    "\n",
    "---\n",
    "\n",
    "## Part 12: Practical Usage\n",
    "\n",
    "### Training Your Own Embeddings\n",
    "\n",
    "**When to train**:\n",
    "- Domain-specific corpus (medical, legal, technical)\n",
    "- Non-English languages\n",
    "- Lots of training data (millions of sentences)\n",
    "\n",
    "**Libraries**:\n",
    "- Gensim (Python): Easy, well-documented\n",
    "- FastText (C++/Python): Fast, handles OOV\n",
    "- TensorFlow/PyTorch: Full control\n",
    "\n",
    "### Using Pre-trained Embeddings\n",
    "\n",
    "**Popular sources**:\n",
    "- Google's Word2Vec (3M words, 300d)\n",
    "- GloVe (various sizes)\n",
    "- FastText (157 languages)\n",
    "\n",
    "**How to use**:\n",
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained\n",
    "model = KeyedVectors.load_word2vec_format('vectors.bin', binary=True)\n",
    "\n",
    "# Get embedding\n",
    "vec = model['cat']  # numpy array (300,)\n",
    "\n",
    "# Find similar\n",
    "similar = model.most_similar('cat', topn=10)\n",
    "# Returns: [('dog', 0.87), ('kitten', 0.82), ...]\n",
    "\n",
    "# Analogy\n",
    "result = model.most_similar(positive=['woman', 'king'], \n",
    "                            negative=['man'], topn=1)\n",
    "# Returns: [('queen', 0.71)]\n",
    "```\n",
    "\n",
    "### Fine-tuning Embeddings\n",
    "\n",
    "Start with pre-trained, continue training on your corpus:\n",
    "- Keeps general knowledge\n",
    "- Adapts to your domain\n",
    "- Requires less data than training from scratch\n",
    "\n",
    "---\n",
    "\n",
    "## Part 13: Evaluation\n",
    "\n",
    "### Intrinsic Evaluation\n",
    "\n",
    "**Analogy tasks**:\n",
    "- Semantic: king:queen :: man:woman\n",
    "- Syntactic: walk:walking :: swim:swimming\n",
    "\n",
    "**Similarity tasks**:\n",
    "- Correlation with human judgments\n",
    "- WordSim-353, SimLex-999 datasets\n",
    "\n",
    "### Extrinsic Evaluation\n",
    "\n",
    "**Does it help downstream tasks?**\n",
    "- Text classification\n",
    "- Named entity recognition\n",
    "- Sentiment analysis\n",
    "- Machine translation\n",
    "\n",
    "**This is what ultimately matters!**\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: The Big Picture\n",
    "\n",
    "Word2Vec revolutionized NLP by showing that:\n",
    "\n",
    "1. **Neural networks can learn semantic representations** automatically from raw text\n",
    "2. **Local context is enough** - don't need global statistics\n",
    "3. **Simple objectives work** - predict context from word, or word from context\n",
    "4. **Approximations are necessary** - negative sampling makes it practical\n",
    "5. **Vector arithmetic captures relationships** - geometry reflects semantics\n",
    "\n",
    "**The paradigm shift**: From hand-crafted features to learned representations.\n",
    "\n",
    "**The foundation**: Word2Vec paved the way for modern NLP (BERT, GPT, etc.)\n",
    "\n",
    "**The principle**: \"A word is characterized by the company it keeps\" - this distributional hypothesis, implemented through neural networks, unlocked machines' ability to understand language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd283e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
